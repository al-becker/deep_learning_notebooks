{
 "cells": [
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mD:\\Programming\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m           \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn_dll_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programming\\Anaconda3\\envs\\deep_learning\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] Das angegebene Modul wurde nicht gefunden",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a62a8722288a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Num GPUs Available: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programming\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programming\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programming\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Perform pre-load sanity checks in order to produce a more actionable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# than we get from an error during SWIG import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mself_check\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreload_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programming\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m               \u001b[1;34m\"necessary DLL by downloading cuDNN %s from this URL: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m               \u001b[1;34m\"https://developer.nvidia.com/cudnn\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m               % (build_info.cudnn_dll_name, build_info.cudnn_version_number))\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Could not find 'cudnn64_8.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7 from this URL: https://developer.nvidia.com/cudnn"
     ],
     "ename": "ImportError",
     "evalue": "Could not find 'cudnn64_8.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7 from this URL: https://developer.nvidia.com/cudnn",
     "output_type": "error"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Parameters"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "num_classes = 10\n",
    "min_width = 224\n",
    "min_height = 224\n",
    "batch_size=8"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "data preparations"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "training_data = Path('../input/10-monkey-species/training/training/') \n",
    "validation_data = Path('../input/10-monkey-species/validation/validation/') \n",
    "labels_path = Path('../input/10-monkey-species/monkey_labels.txt')\n",
    "\n",
    "\n",
    "labels_info = []\n",
    "\n",
    "# Read and cleanup the file\n",
    "lines = labels_path.read_text().strip().splitlines()\n",
    "for line in lines[1:]:\n",
    "    line = line.split(',')\n",
    "    line = [x.strip(' \\n\\t\\r') for x in line]\n",
    "    line[3], line[4] = int(line[3]), int(line[4])\n",
    "    line = tuple(line)\n",
    "    labels_info.append(line)\n",
    "    \n",
    "# Convert the data into a pandas dataframe\n",
    "labels_info = pd.DataFrame(labels_info, columns=['Label', 'Latin Name', 'Common Name', \n",
    "                                                 'Train Images', 'Validation Images'], index=None)\n",
    "# Show Labels\n",
    "labels_info.head(10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "# map labels to common names\n",
    "names_dict = dict(zip(labels_info.index, labels_info[\"Common Name\"]))\n",
    "print(names_dict)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "data_generator_with_aug = keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                                       horizontal_flip=True,\n",
    "                                                                       width_shift_range=0.2,\n",
    "                                                                       height_shift_range=0.2,\n",
    "                                                                       rotation_range=30)\n",
    "\n",
    "data_generator_no_aug = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = data_generator_with_aug.flow_from_directory('../input/10-monkey-species/training/training',\n",
    "                                                              target_size=(min_width, min_height),\n",
    "                                                              batch_size=batch_size,\n",
    "                                                              shuffle=True,\n",
    "                                                              class_mode='categorical')\n",
    "validation_generator = data_generator_no_aug.flow_from_directory('../input/10-monkey-species/validation/validation',\n",
    "                                                                 target_size=(min_width, min_height),\n",
    "                                                                 batch_size=batch_size,\n",
    "                                                                 shuffle=False,\n",
    "                                                                 class_mode='categorical')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "training a model from scratch\n",
    "- is super slow (most likely reading and transforming the images is a big bottleneck)"
   ]
  },
  {
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "model_from_scratch = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(min_width, min_height, 3)),\n",
    "        keras.layers.Conv2D(32,kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        keras.layers.Conv2D(64,kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_from_scratch.compile(optimizer='rmsprop',\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "model_from_scratch.summary()\n",
    "\n",
    "history = model_from_scratch.fit_generator(train_generator,\n",
    "                                 epochs=200,\n",
    "                                 validation_data=validation_generator)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "training a model based on xception"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "model_xception = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(min_width, min_height, 3)),\n",
    "        keras.applications.Xception(weights=\"imagenet\", include_top=False),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(512, activation='relu'),\n",
    "        #keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_xception.compile(optimizer='rmsprop',\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "model_xception.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "history = model_xception.fit_generator(train_generator,\n",
    "                                 epochs=35,\n",
    "                                 validation_data=validation_generator)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "model_from_scratch.save('xception') "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "df_acc = DataFrame (acc, columns=['acc'])\n",
    "df_acc['val_acc'] = history.history['val_accuracy']\n",
    "df_acc.index += 1\n",
    "\n",
    "loss = history.history['loss']\n",
    "df_loss = DataFrame (loss, columns=['loss'])\n",
    "df_loss['val_loss'] = history.history['val_loss']\n",
    "df_loss.index += 1\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.title(\"accuracy of the model\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "sns.lineplot(data=df_acc)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.title(\"loss of the model\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "sns.lineplot(data=df_loss)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict the values from the validation datasetand convert them to one hot encoded vectors\n",
    "y_pred = np.argmax(model_from_scratch.predict_generator(validation_generator, validation_generator.samples), axis = 1)\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_true = validation_generator.classes, y_pred = y_pred)\n",
    "# plot the confusion matrix\n",
    "#plot_confusion_matrix(confusion_mtx, normalize=True, target_names=labels)\n",
    "sns.heatmap(data=confusion_mtx, annot=True, cmap='rocket_r')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}